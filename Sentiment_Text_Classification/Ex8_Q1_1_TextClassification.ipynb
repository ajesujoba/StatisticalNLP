{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Ex8_Q1_1_TextClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl8Aw0TJY5Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk import ngrams\n",
        "import pandas as pd\n",
        "import glob as gb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ak5p7Hzfb0",
        "colab_type": "text"
      },
      "source": [
        "1a starts here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYM4NM4YiIBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method does the tokenization and removal of numbers and punctuations\n",
        "def tokenize(text):\n",
        "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
        "    return re.findall('[a-z]+', text.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbyl26cFf1FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method handles reading the training corpus\n",
        "def readCorpus(directory=\"ag_news_csv_cleaned/train_cleaned.csv\"):\n",
        "  with open(directory, 'r',encoding=\"utf8\", errors='replace') as doc:\n",
        "      data = doc.readlines()\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvfStW_ehxrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read corpus method is invoked here\n",
        "training_data=readCorpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QguUM7gCqldE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#divide sentence to their categories 1,2,3,4 they are saved respectively to list in index 0,1,2,3 which is a list of lists\n",
        "catSentList=[[],[],[],[]]\n",
        "for x in training_data:\n",
        "  catSentList[int(x[1:2])-1].append(tokenize(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KnpruBHh-dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge category list gotten above to a universal list for each category\n",
        "import more_itertools\n",
        "reducedList=[]\n",
        "for i in range(len(catSentList)):\n",
        "  reducedList.append(list(more_itertools.flatten(catSentList[i])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmbQaIuYrAxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#count the words in each group/category\n",
        "countDictList=[]\n",
        "for i in range(len(reducedList)):\n",
        "  countDictList.append(Counter(reducedList[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndhG-NbSiuNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this takes the training data as a whole instead of filtering and grouping them \n",
        "#universal count\n",
        "listofSentList=[]\n",
        "for x in training_data:\n",
        "  listofSentList.append(tokenize(x))\n",
        "#convert the list of list into just one which is a universal list\n",
        "flatSentList=list(more_itertools.flatten(listofSentList))\n",
        "#count word occurence in the list and remove words that have counts less than 3\n",
        "sentCount=Counter(flatSentList)\n",
        "vocabulary=Counter(dict(filter(lambda x: x[1] >= 2, sentCount.items())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_p00CUzXTu",
        "colab_type": "text"
      },
      "source": [
        "1a ends here and 1b starts here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Y1nT5i5jea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b From the preprocessed text, collect the following statistics\n",
        "D=len(listofSentList) #the total number of documents\n",
        "Dk= [len(catSentList[0]),len(catSentList[1]),len(catSentList[2]),len(catSentList[3]) ] #the total number of documents labelled with class k+1\n",
        "#the frequency of word wt in the documents of class k\n",
        "dictVocab = dict(vocabulary)\n",
        "vocabularyCat = []\n",
        "#copy the vabulary and replicate it for each categories\n",
        "for i in range(len(Dk)):\n",
        "  vocabularyCat.append(dictVocab.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVF_064KTLQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute the N_K for each category, that is the frequency of word w t in the documents of class k\n",
        "for i in range(len(vocabularyCat)):\n",
        "  for k,v in vocabularyCat[i].items(): \n",
        "    vocabularyCat[i][k] = countDictList[i][k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFVnkJ677lQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# c Estimate the priors P(C_k) \n",
        "priorsPCK = [] \n",
        "catLength = [] \n",
        "for i in range(len(Dk)):\n",
        "  priorsPCK.append(Dk[i]/D)\n",
        "  catLength.append(sum(vocabularyCat[i].values()))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfZZH3xK84Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d Estimate the likelihoods P(w_t|C_k) as\n",
        "def Likelihood(word,CK=1,alpha=1):\n",
        "  likelihd = (alpha + countDictList[CK-1][word])/(len(vocabulary) + catLength[CK-1])\n",
        "  return likelihd\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BgGLsBTL4Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Design and implement a Python structure NB_Classifier that encapsulates the model\n",
        "# parameters mentioned above, namely the priors and the likelihoods.\n",
        "def NB_Classifier2(wordseq):\n",
        "  classCat = []\n",
        "  for i in range(len(Dk)):\n",
        "    firstPart = math.log(priorsPCK[i],2)\n",
        "    #the second part is a sum over \n",
        "    secondpart = sum([math.log(Likelihood(j,CK = i+1),2) for j in wordseq])\n",
        "    classCat.append(firstPart+secondpart)\n",
        "  return classCat.index(max(classCat))+1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqu4-C18QVls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method handles reading the training corpus\n",
        "def readTestCorpus(directory=\"ag_news_csv_cleaned/test_cleaned.csv\"):\n",
        "  with open(directory, 'r',encoding=\"utf8\", errors='replace') as doc:\n",
        "      data = doc.readlines()\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81p21RPLQVvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read test corpus method is invoked here\n",
        "test_data=readTestCorpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTePJfJqQV4q",
        "colab_type": "code",
        "outputId": "ed06dacf-54c6-44af-cd0d-a0f2a20f88a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ny111seZ6BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# e Apply the same prepossessing steps that you have used on the training portion on the test\n",
        "#portion. Using your method in part (e), predict the class for every test document.\n",
        "classlabel = []\n",
        "predlabel = []\n",
        "testSet = []\n",
        "for x in test_data:\n",
        "  classlabel.append(int(x[1:2]))\n",
        "  testSet.append(tokenize(x))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKVJ2F-0NLcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate the predicted classes per document\n",
        "for document in testSet:\n",
        "  predlabel.append(NB_Classifier2(document))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpvTsinicD5L",
        "colab_type": "code",
        "outputId": "6da66915-e99b-4751-c7d8-0ae09cf27b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#calculate the accuracy, get the number of times the lists matches \n",
        "correctPred=sum(a == b for a,b in zip(classlabel, predlabel))\n",
        "print(\"correct predictions = \",correctPred)\n",
        "accuracy =  correctPred/len(classlabel)\n",
        "print(\"The accuracy is = \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct predictions =  6822\n",
            "The accuracy is =  0.8976315789473684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEQ6DF5YdW9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confusion matrix\n",
        "import pandas as pd\n",
        "y_actu = pd.Series(classlabel, name='Actual')\n",
        "y_pred = pd.Series(predlabel, name='Predicted')\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hFWWu5y9_ir",
        "colab_type": "code",
        "outputId": "0987dc84-f6a5-41f0-c4a0-0727c287c457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df_confusion"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1697</td>\n",
              "      <td>63</td>\n",
              "      <td>92</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25</td>\n",
              "      <td>1854</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>19</td>\n",
              "      <td>1599</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>18</td>\n",
              "      <td>140</td>\n",
              "      <td>1672</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted     1     2     3     4\n",
              "Actual                           \n",
              "1          1697    63    92    48\n",
              "2            25  1854     7    14\n",
              "3            70    19  1599   212\n",
              "4            70    18   140  1672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZNl8y0sec1Y",
        "colab_type": "text"
      },
      "source": [
        "3 and 4 get confused more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXlKNRcX_RNp",
        "colab_type": "text"
      },
      "source": [
        " Design and implement a Python structure NB_Classifier that encapsulates the model\n",
        "parameters mentioned above, namely the priors and the likelihoods.\n",
        "def NB_Classifier(wordseq):\n",
        "  classCat = []\n",
        "  for i in range(len(Dk)):\n",
        "    firstPart = math.log(priorsPCK[i],2)\n",
        "    #the second part is a sum over \n",
        "    secondpart=0\n",
        "    for j in wordseq:\n",
        "      secondpart+=math.log(Likelihood(j,CK = i+1),2)\n",
        "    classCat.append(firstPart+secondpart)\n",
        "  return classCat.index(max(classCat))+1"
      ]
    }
  ]
}