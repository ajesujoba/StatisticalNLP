{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Ex8_Q1_2_SentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl8Aw0TJY5Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import the libraries\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk import ngrams\n",
        "import pandas as pd\n",
        "import glob as gb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import more_itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYM4NM4YiIBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method does the tokenization and removal of numbers and punctuations\n",
        "def tokenize(text):\n",
        "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
        "    return re.findall('[a-z]+', text.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbyl26cFf1FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method handles reading the training corpus\n",
        "def readCorpus(directory=\"imdb_movie_reviews/imdb_dataset.csv\"):\n",
        "  with open(directory, 'r',encoding=\"utf8\", errors='replace') as doc:\n",
        "      data = doc.readlines()\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvfStW_ehxrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read corpus method is invoked here\n",
        "training_data=readCorpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAM6tZc-3c52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KfoldSplit(traing_data, k_chunk_size=5000):\n",
        "  #shuffle the data randomly\n",
        "  random.shuffle(training_data) #shuffle method\n",
        "  #training data is a list of list\n",
        "  training_dataClean = []\n",
        "  #training_dataClean is a list of tuples (class, document)\n",
        "  for x in training_data:\n",
        "    training_dataClean.append((x[-2:-1],tokenize(x)))\n",
        "  return [training_dataClean[offs:offs+k_chunk_size] for offs in range(0, len(training_dataClean), k_chunk_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp4OJCfy6B0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splittedDataset1=KfoldSplit(training_data, k_chunk_size=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8rtoVzi6dJa",
        "colab_type": "code",
        "outputId": "e7c3785a-4006-454d-d94d-a9aac1b5aad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('the total number of folds = ',len(splittedDataset1))\n",
        "print('fold 1 has  = ',len(splittedDataset1[0]))\n",
        "#they are splitted to 10 folds of 5000 documents each \n",
        "#thus 9 would be merged for training and a fold for testing 10 times"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the total number of folds =  10\n",
            "fold 1 has  =  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx82fSvI8X7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the different combinations of folds\n",
        "def getFolds(splittedDataset):\n",
        "  testDataGroups = []\n",
        "  trainingDataGroups = []\n",
        "  for groups in range(len(splittedDataset)):\n",
        "    testDataGroups.append(splittedDataset[groups])\n",
        "    trainingDataGroups.append(splittedDataset[:groups]+splittedDataset[groups+1:])\n",
        "  #merge the 9 lists per trainingDataGroups\n",
        "  for i in range(len(trainingDataGroups)):\n",
        "    trainingDataGroups[i] = list(itertools.chain.from_iterable(trainingDataGroups[i]))\n",
        "  return testDataGroups, trainingDataGroups\n",
        "# each fold exist as the test group onnce making the size of each just 5000 while for each instance of a testgroup, other groups are combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwmESm485FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testDataGroups1, trainingDataGroups1 = getFolds(splittedDataset1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUOpwXPu9PFN",
        "colab_type": "code",
        "outputId": "2cced693-0d4a-4984-9140-ab1cce69228e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print('the total number of folds = ',len(testDataGroups1))\n",
        "print('the total number test set per folds (fold 1) = ',len(testDataGroups1[0]))\n",
        "print('the total number of training set per folds (fold 1) = ',len(trainingDataGroups1[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the total number of folds =  10\n",
            "the total number test set per folds (fold 1) =  5000\n",
            "the total number of training set per folds (fold 1) =  45000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3_obIwN-Qv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this method encapsulates what we had in 1.1, codes that was used . This is everytime a model is to be trainned\n",
        "def processTheGroup(trainingDataGroups):\n",
        "  \n",
        "  #divide sentence to their categories 1,2 they are saved respectively to list in index 0,1 which is a list of lists\n",
        "  catSentList=[[],[]]\n",
        "  for x in trainingDataGroups:\n",
        "    catSentList[int(x[0])-1].append(x[1])\n",
        "  \n",
        "  #merge category list gotten above to a universal list for each category; \n",
        "  #catSentList is a list of lists, to merge the list we did the following\n",
        "  reducedList=[]\n",
        "  for i in range(len(catSentList)):\n",
        "    reducedList.append(list(more_itertools.flatten(catSentList[i])))\n",
        "    \n",
        "  #count the words in each group/category\n",
        "  countDictList=[]\n",
        "  for i in range(len(reducedList)):\n",
        "    countDictList.append(Counter(reducedList[i]))\n",
        "  \n",
        "  #this takes the training data as a whole instead of filtering and grouping them \n",
        "  #comibine the two classes into one (merge the list for class 1 & 2) and removing words with frequency less than 2\n",
        "  sentCount=countDictList[0]+countDictList[1] \n",
        "  vocabulary=Counter(dict(filter(lambda x: x[1] >= 2, sentCount.items())))\n",
        "  return catSentList,countDictList,vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW6rDEYg-5sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collectStatistics(catSentList,vocabulary):\n",
        "  # b From the preprocessed text, collect the following statistics\n",
        "  D = len(catSentList[0])+len(catSentList[1])#the total number of documents\n",
        "  \n",
        "  Dk = [len(catSentList[0]),len(catSentList[1])] #the total number of documents labelled with class k+1\n",
        "  \n",
        "  #the frequency of word wt in the documents of class k\n",
        "  dictVocab = dict(vocabulary)\n",
        "  vocabularyCat=[]\n",
        "  for i in range(len(Dk)):\n",
        "    vocabularyCat.append(dictVocab.copy())\n",
        "  for i in range(len(vocabularyCat)):\n",
        "    for k,v in vocabularyCat[i].items(): \n",
        "      vocabularyCat[i][k] = countDictList[i][k]\n",
        "  return D, Dk,vocabularyCat \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z10asnRbnGGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d Estimate the likelihoods P(w_t|C_k) as\n",
        "def Likelihood(word,CK=1,alpha=1):\n",
        "  likelihd = (alpha + countDictList[CK-1][word])/(len(vocabulary) + catLength[CK-1])\n",
        "  return likelihd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dwcTooyno10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Prior():\n",
        "  # c Estimate the priors P(C_k) \n",
        "  priorsPCK = [] \n",
        "  catLength = [] \n",
        "  for i in range(len(Dk)):\n",
        "    priorsPCK.append(Dk[i]/D)\n",
        "    catLength.append(sum(vocabularyCat[i].values()))\n",
        "  return catLength,priorsPCK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B3xBtfwmCxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Design and implement a Python structure NB_Classifier that encapsulates the model\n",
        "# parameters mentioned above, namely the priors and the likelihoods.\n",
        "def NB_Classifier(wordseq):\n",
        "  classCat = []\n",
        "  for i in range(len(Dk)):\n",
        "    firstPart = math.log(priorsPCK[i],2)\n",
        "    #the second part is a sum over \n",
        "    secondpart = sum([math.log(Likelihood(j,CK = i+1),2) for j in wordseq])\n",
        "    classCat.append(firstPart+secondpart)\n",
        "  return classCat.index(max(classCat))+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0llE2LZ11B3t",
        "colab_type": "code",
        "outputId": "2fe83f2b-533a-4c3d-9ad4-9eb8eac9b471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "#first fold\n",
        "accuracyList=[]\n",
        "for i in range(10):\n",
        "  catSentList,countDictList,vocabulary =processTheGroup(trainingDataGroups1[i])\n",
        "  D, Dk,vocabularyCat=collectStatistics(catSentList,vocabulary)\n",
        "  print\n",
        "  catLength,priorsPCK=Prior()\n",
        "  \n",
        "  classlabel = []; text_data=[]; predlabel = []\n",
        "  for x in testDataGroups1[i]:\n",
        "    classlabel.append(int(x[0]))\n",
        "    text_data.append(x[1])\n",
        "  #generate the predicted classes per document\n",
        "  for document in text_data:\n",
        "    predlabel.append(NB_Classifier(document))\n",
        "  #calculate the accuracy, get the number of times the lists matches \n",
        "  correctPred=sum(a == b for a,b in zip(classlabel, predlabel))\n",
        "  accuracy =  correctPred/len(classlabel)\n",
        "  accuracyList.append(accuracy)\n",
        "  print('Correct prediction = ', correctPred)\n",
        "  print(\"The accuracy is = \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction =  4190\n",
            "The accuracy is =  0.838\n",
            "Correct prediction =  4202\n",
            "The accuracy is =  0.8404\n",
            "Correct prediction =  4231\n",
            "The accuracy is =  0.8462\n",
            "Correct prediction =  4223\n",
            "The accuracy is =  0.8446\n",
            "Correct prediction =  4213\n",
            "The accuracy is =  0.8426\n",
            "Correct prediction =  4251\n",
            "The accuracy is =  0.8502\n",
            "Correct prediction =  4214\n",
            "The accuracy is =  0.8428\n",
            "Correct prediction =  4227\n",
            "The accuracy is =  0.8454\n",
            "Correct prediction =  4280\n",
            "The accuracy is =  0.856\n",
            "Correct prediction =  4240\n",
            "The accuracy is =  0.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1XW_b7lYzUR",
        "colab_type": "code",
        "outputId": "47a2886f-ae74-419c-e0ce-86bac4dc0efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(accuracyList)\n",
        "print(\"The average accuracy = \", round(np.mean(accuracyList),4))\n",
        "print(\"The standard deviation = \", round(np.std(accuracyList),4))\n",
        "print(\"The maximum accuracy = \", np.max(accuracyList))\n",
        "print(\"The minimum accuracy = \", np.min(accuracyList))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.838, 0.8404, 0.8462, 0.8446, 0.8426, 0.8502, 0.8428, 0.8454, 0.856, 0.848]\n",
            "The average accuracy =  0.8454\n",
            "The standard deviation =  0.0049\n",
            "The maximum accuracy =  0.856\n",
            "The minimum accuracy =  0.838\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}